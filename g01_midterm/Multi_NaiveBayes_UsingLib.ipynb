{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data_fn, label_fn, nwords):\n",
    "    \n",
    "    with open(nwords, 'r', encoding = 'utf-8') as f:\n",
    "        content = f.readlines()\n",
    "    nwords = [int(x.strip()) for x in content][0]\n",
    "    ## read label_fn\n",
    "    with open(label_fn, 'r', encoding = 'utf-8') as f:\n",
    "        content = f.readlines()\n",
    "    label = [int(x.strip()) for x in content]\n",
    "    ## read data_fn\n",
    "    with open (data_fn, 'r', encoding = 'utf-8') as f:\n",
    "        content = f.readlines()\n",
    "    # remove ’\\n’ at the end of each line\n",
    "    content = [x.strip() for x in content]\n",
    "    dat = np.zeros((len(content), 3), dtype = int)\n",
    "    for i, line in enumerate(content):\n",
    "        a = line.split(' ')\n",
    "        dat[i, :] = np.array([int(a[0]), int(a[1]), int(a[2])])\n",
    "    # remember to -1 at coordinate since we’re in Python\n",
    "    data = coo_matrix((dat[:, 2], (dat[:, 0] , dat[:, 1] )),shape=(len(label), nwords))\n",
    "    return (data, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length dict = 1888 . accuracy_test_folder1 = 67.5\n",
      "Length dict = 1888 . accuracy_train_folder1 = 89.38906752411575\n",
      "----------------------------------------\n",
      "Length dict = 1967 . accuracy_test_folder2 = 67.5\n",
      "Length dict = 1967 . accuracy_train_folder2 = 91.31832797427653\n",
      "----------------------------------------\n",
      "Length dict = 1875 . accuracy_test_folder3 = 62.5\n",
      "Length dict = 1875 . accuracy_train_folder3 = 89.06752411575563\n",
      "----------------------------------------\n",
      "Length dict = 1867 . accuracy_test_folder4 = 56.25\n",
      "Length dict = 1867 . accuracy_train_folder4 = 91.31832797427653\n",
      "----------------------------------------\n",
      "Length dict = 1929 . accuracy_test_folder5 = 66.19718309859155\n",
      "Length dict = 1929 . accuracy_train_folder5 = 90.34267912772586\n",
      "----------------------------------------\n",
      "average accurracy_train :  0.9028718534323005\n",
      "average accurracy_test :  0.6398943661971831\n"
     ]
    }
   ],
   "source": [
    "sum_acc_test = 0\n",
    "aver_acc_test = 0\n",
    "sum_acc_train = 0\n",
    "aver_acc_train = 0\n",
    "for i in range(1, 6):\n",
    "    train_data_fn = 'data_thua/train_fn{}.txt'.format(i)\n",
    "    train_label_fn = 'data_thua/train_label{}.txt'.format(i)\n",
    "    test_data_fn = 'data_thua/test_fn{}.txt'.format(i)\n",
    "    test_label_fn = 'data_thua/test_label{}.txt'.format(i)\n",
    "    nwords = 'data_thua/nword{}.txt'.format(i)\n",
    "    (train_data, train_label) = read_data(train_data_fn, train_label_fn, nwords)\n",
    "    (test_data, test_label) = read_data(test_data_fn, test_label_fn, nwords)\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(train_data, train_label)\n",
    "    y_pred_test = clf.predict(test_data)\n",
    "    \n",
    "    y_pred_train = clf.predict(train_data)\n",
    "    \n",
    "    acc_test = accuracy_score(y_pred_test,test_label)\n",
    "    \n",
    "    acc_train = accuracy_score(y_pred_train,train_label)\n",
    "    \n",
    "    sum_acc_test = sum_acc_test + acc_test\n",
    "    sum_acc_train = sum_acc_train + acc_train\n",
    "    \n",
    "    print('Length dict =',train_data.shape[1],'. accuracy_test_folder{} ='.format(i), acc_test*100)\n",
    "    \n",
    "    print('Length dict =',train_data.shape[1],'. accuracy_train_folder{} ='.format(i), acc_train*100)\n",
    "    \n",
    "    print('----------------------------------------')\n",
    "\n",
    "aver_acc_train = sum_acc_train/5\n",
    "aver_acc_test = sum_acc_test/5\n",
    "print('average accurracy_train : ', aver_acc_train)\n",
    "print('average accurracy_test : ', aver_acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(X_train, y_train):\n",
    "    N,D = X_train.shape\n",
    "    classes, noElementOfClass = np.unique(y_train, return_counts=True)\n",
    "    K=classes.size\n",
    "    theta = np.ones((K,1))\n",
    "    mu_jk = np.ones((K,D))\n",
    "    sigma_jk = np.ones((K,D))\n",
    "    for k in range(0,K):\n",
    "        theta[k]= noElementOfClass[k] \n",
    "        ide = np.where(y_train==classes[k])[-1]\n",
    "        X=[]\n",
    "        for i in ide:\n",
    "             X.append(X_train[i][:])\n",
    "        X=np.array(X)\n",
    "        mu_jk[k][:]= np.sum(X,0)/X.shape[0]\n",
    "        sigma_jk[k][:]= np.square(X[k][:]-mu_jk[k][:])/X.shape[0]\n",
    "    theta/=N\n",
    "    return theta, mu_jk, sigma_jk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(theta, mu_jk, sigma_jk, x):\n",
    "    K,D = mu_jk.shape\n",
    "    p= np.ones((K,1))\n",
    "    for k in range(0, K):\n",
    "        l=np.log(1/np.sqrt(2*np.pi*sigma_jk[k][:]))-(np.square(x[:]-mu_jk[k][:])/(2*sigma_jk[k][:]))\n",
    "        p[k] = np.log(theta[k])+np.sum(l)\n",
    "    return np.argmax(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "def loss(w, X, y, lam):\n",
    "    z = prob(w,X)\n",
    "    return -np.mean(y*np.log(z) + (1-y)*np.log(1-z) + 0.5*lam/X.shape[0]*np.sum(w*w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(theta, mu_jk, sigma_jk,X, y):\n",
    "    K, D = sigma_jk.shape\n",
    "    N, D  = X.shape\n",
    "    y_pre = np.ones((N,1))\n",
    "    count =0;\n",
    "    for j in range(N):\n",
    "        y_pre[j] = classify(theta, mu_jk, sigma_jk, X[j][:])\n",
    "        if y_pre[j]==y[j]:\n",
    "            count+=1\n",
    "    return count/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of dict folder 1:  1888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  \"\"\"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \"\"\"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in subtract\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accurracy_test of folder 1:  0.5125 \n",
      "accurracy_train of folder 1:  0.5176848874598071\n",
      "len of dict folder 2:  1967\n",
      "accurracy_test of folder 2:  0.55 \n",
      "accurracy_train of folder 2:  0.5112540192926045\n",
      "len of dict folder 3:  1875\n",
      "accurracy_test of folder 3:  0.4 \n",
      "accurracy_train of folder 3:  0.5434083601286174\n",
      "len of dict folder 4:  1867\n",
      "accurracy_test of folder 4:  0.65 \n",
      "accurracy_train of folder 4:  0.4855305466237942\n",
      "len of dict folder 5:  1929\n",
      "accurracy_test of folder 5:  0.4647887323943662 \n",
      "accurracy_train of folder 5:  0.5264797507788161\n",
      "average accurracy_train :  0.5168715128567278\n",
      "average accurracy_test :  0.5154577464788732\n"
     ]
    }
   ],
   "source": [
    "sum_acc_test = 0\n",
    "aver_acc_test = 0\n",
    "sum_acc_train = 0\n",
    "aver_acc_train = 0\n",
    "for i in range(1, 6):\n",
    "    A = np.loadtxt('data_thua/train{}.txt'.format(i))\n",
    "    X_train = np.array(A[:, 1: ])\n",
    "    y_train = np.array(A[:, 0])\n",
    "    print('len of dict folder {}: '.format(i), X_train.shape[1])\n",
    "    T = np.loadtxt('data_thua/test{},txt'.format(i))\n",
    "    X_test=np.array(T[:, 1: ])\n",
    "    y_test=np.array(T[:, 0])\n",
    "    theta, mu_jk, sigma_jk=training(X_train, y_train)\n",
    "    \n",
    "    acc_test = accuracy(theta, mu_jk, sigma_jk,X_test, y_test)\n",
    "    \n",
    "    acc_train = accuracy(theta, mu_jk, sigma_jk,X_train, y_train)\n",
    "    \n",
    "    sum_acc_test = sum_acc_test + acc_test\n",
    "    sum_acc_train = sum_acc_train + acc_train\n",
    "    \n",
    "    print('accurracy_test of folder {}: '.format(i), acc_test, '')\n",
    "    print('accurracy_train of folder {}: '.format(i), acc_train)\n",
    "    \n",
    "aver_acc_train = sum_acc_train/5\n",
    "aver_acc_test = sum_acc_test/5\n",
    "\n",
    "print('average accurracy_train : ', aver_acc_train)\n",
    "print('average accurracy_test : ', aver_acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
